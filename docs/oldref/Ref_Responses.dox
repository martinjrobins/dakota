namespace Dakota {

/** \page RespCommands Responses Commands

\htmlonly
<b>Responses Commands Table of Contents</b>
<ul>
<li> <a href="RespCommands.html#RespDescr">Responses Description</a>
<li> <a href="RespCommands.html#RespSpec">Responses Specification</a>
<li> <a href="RespCommands.html#RespSetId">Responses Set Identifier</a>
<li> <a href="RespCommands.html#RespLabels">Response Labels</a>
<li> <a href="RespCommands.html#RespFn">Function Specification</a>
  <ul>
  <li> <a href="RespCommands.html#RespFnOpt">Objective and constraint 
       functions (optimization data set)</a>
  <li> <a href="RespCommands.html#RespFnLS">Calibration terms and 
       constraint functions (least squares data set)</a>
  <li> <a href="RespCommands.html#RespFnGen">Response functions 
       (generic data set)</a>
  </ul>
<li> <a href="RespCommands.html#RespGrad">Gradient Specification</a>
  <ul>
  <li> <a href="RespCommands.html#RespGradNone">No gradients</a>
  <li> <a href="RespCommands.html#RespGradNum">Numerical gradients</a>
  <li> <a href="RespCommands.html#RespGradAnalytic">Analytic gradients</a>
  <li> <a href="RespCommands.html#RespGradMixed">Mixed gradients</a>
  </ul>
<li> <a href="RespCommands.html#RespHess">Hessian Specification</a>
  <ul>
  <li> <a href="RespCommands.html#RespHessNone">No Hessians</a>
  <li> <a href="RespCommands.html#RespHessNum">Numerical Hessians</a>
  <li> <a href="RespCommands.html#RespHessQuasi">Quasi Hessians</a>
  <li> <a href="RespCommands.html#RespHessAnalytic">Analytic Hessians</a>
  <li> <a href="RespCommands.html#RespHessMixed">Mixed Hessians</a>
  </ul>
</ul>
\endhtmlonly


\section RespDescr Responses Description


Responses specify the data set produced by an interface after the
completion of a "function evaluation."  Here, the term function
evaluation is used loosely to denote a data request from an iterator
that is mapped through an interface in a single pass.  Strictly
speaking, this data request may actually involve multiple response
functions and their derivatives, but the term function evaluation is
widely used for this purpose.  The data set is potentially comprised
of a set of functions, their first derivative vectors (gradients), and
their second derivative matrices (Hessians). This abstraction provides
a generic data container (the Response class) whose contents are
interpreted differently depending upon the type of iteration being
performed. In the case of optimization, the set of functions consists
of one or more objective functions, nonlinear inequality constraints,
and nonlinear equality constraints. (Linear constraints are not part of
a response set since their coefficients can be communicated to an
optimizer at start up and then computed internally for all function
evaluations; see \ref MethodIndControl). In the case of least squares
iterators, the functions consist of individual residual terms or model
responses together with an observed data file for comparison (as
opposed to a sum of the squares objective function) as well as
nonlinear inequality and equality constraints. In the case of
nondeterministic iterators, the function set is made up of generic
response functions for which the effect of parameter uncertainty is to
be quantified. Parameter study and design of experiments iterators may
be used with any of the response data set types. Thus the
interpretation of the response data varies from iterator to iterator.

Gradient specification types include none, numerical, analytic, and
mixed.  The \c no_gradients selection indicates that gradient
information is not needed in the study. The \c numerical_gradients
selection means that gradient information is needed and will be
computed with finite differences by %Dakota or the optimization
algorithm in use.  The \c analytic_gradients selection means that
gradient information is available directly from the simulation (finite
differencing is not required). And the \c mixed_gradients selection
means that some gradient information is available directly from the
simulation whereas the rest will have to be estimated with finite
differences.

Hessian availability is characterized as none, analytic, numerical,
quasi, or mixed.  Similar to gradients, the \c no_hessians selection
indicates that Hessian information is not needed/available in the
study, and the \c analytic_hessians selection indicates that Hessian
information is available directly from the simulation.  The \c
numerical_hessians selection indicates that Hessian information will
be estimated with finite differences.  The \c quasi_hessians
specification means that Hessian information will be accumulated over
time using secant updates based on the existing gradient evaluations.
Finally, the \c mixed_hessians selection allows for a mixture of
analytic, numerical, and quasi Hessian response data.

Responses specify the \e total data set that is available for use by
the method over the course of iteration. This is distinguished from
the data \e subset described by an active set vector (see %Dakota File
Data Formats in the Users Manual [\ref UsersMan "Adams et al., 2010"])
indicating the particular subset of the response data needed for a
particular function evaluation. Thus, the responses specification is a
broad description of the data to be used during a study whereas the
active set vector indicates the subset currently needed.

Several examples follow. The first example shows an optimization data
set containing an objective function and two nonlinear inequality
constraints. These three functions have analytic gradient availability
and no Hessian availability.

\verbatim
responses,
	objective_functions = 1
	nonlinear_inequality_constraints = 2
	analytic_gradients
	no_hessians
\endverbatim

The next example shows a typical specification for a calibration data
set. The six residual functions will have numerical gradients computed
using the dakota finite differencing routine with central differences
of 0.1% (plus/minus delta value = .001*value).

\verbatim
responses,
	calibration_terms = 6
	numerical_gradients
	  method_source dakota
	  interval_type central
	  fd_gradient_step_size = .001
	no_hessians
\endverbatim

The last example shows a specification that could be used with a
nondeterministic sampling iterator. The three response functions have no
gradient or Hessian availability; therefore, only function values will
be used by the iterator.

\verbatim
responses,
	response_functions = 3
	no_gradients
	no_hessians
\endverbatim

Parameter study and design of experiments iterators are not restricted
in terms of the response data sets which may be catalogued; they may
be used with any of the function specification examples shown above.


\section RespSpec Responses Specification


The responses specification has the following structure (see
dakota.input.summary):

\verbatim
responses,
	<set identifier>
	<response descriptors>
	<function specification>
	<gradient specification>
	<Hessian specification>
\endverbatim

The set identifier and response descriptors are optional. However, the
function, gradient, and Hessian specifications are all required, their
type selected from the options discussed above.  For example, the
function specification must be one of three types:
  \li objective and constraint functions 
  \li calibration (least squares) terms and constraint functions
  \li generic response functions

The following sections describe each of these specification
components and their options in additional detail.


\section RespSetId Responses Set Identifier


The optional set identifier specification uses the keyword \c
id_responses to input a string for use in identifying a particular
responses specification.  A model can then identify the use of this
response set by specifying the same string in its \c responses_pointer
specification (see \ref ModelIndControl). For example, a model whose
specification contains <tt>responses_pointer = 'R1'</tt> will use a
responses set with <tt>id_responses = 'R1'</tt>.

If the \c id_responses specification is omitted, a particular
responses specification will be used by a model only if that model
omits specifying a \c responses_pointer and if the responses set was
the last set parsed (or is the only set parsed). In common practice,
if only one responses set exists, then \c id_responses can be safely
omitted from the responses specification and \c responses_pointer can
be omitted from the model specification(s), since there is no
potential for ambiguity in this case. \ref T9d1 "Table 9.1" summarizes
the set identifier input.

\anchor T9d1
<table>
<caption align = "top">
\htmlonly
Table 9.1
\endhtmlonly
Specification detail for set identifier
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Responses set identifier
<td>\c id_responses
<td>string
<td>Optional
<td>use of last responses parsed
</table>


\section RespLabels Response Labels

The optional response labels specification \c response_descriptors is
a list of strings which will be printed in %Dakota output to identify
the values for particular response functions.  The default descriptor
strings use a root string plus a numeric identifier.  This root string
is \c "obj_fn" for objective functions, \c "least_sq_term" for least
squares terms, \c "response_fn" for generic response functions, \c
"nln_ineq_con" for nonlinear inequality constraints, and \c
"nln_eq_con" for nonlinear equality constraints.  
\ref T9d2 "Table 9.2" summarizes the response descriptors input.

\anchor T9d2
<table>
<caption align = "top">
\htmlonly
Table 9.2
\endhtmlonly
Specification detail for response labels
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>%Response labels
<td>\c descriptors
<td>list of strings
<td>Optional
<td>root strings plus numeric identifiers
</table>


\section RespFn Function Specification


The function specification must be one of three types: 1) a group
containing objective and constraint functions, 2) a group containing
calibration (least squares) terms and constraint functions, or 3) a
generic response functions specification. These function sets
correspond to optimization, least squares, and uncertainty
quantification iterators, respectively. Parameter study and design of
experiments iterators may be used with any of the three function
specifications.


\subsection RespFnOpt Objective and constraint functions (optimization data set)

An optimization data set is specified using \c objective_functions and
optionally \c sense, \c primary_scale_types, \c primary_scales, 
\c weights, \c nonlinear_inequality_constraints, \c lower_bounds, 
\c upper_bounds, \c nonlinear_equality_constraints, \c targets, 
\c scale_types, and \c scales.  The \c objective_functions, 
\c nonlinear_inequality_constraints, and \c nonlinear_equality_constraints 
inputs specify the number of objective functions, nonlinear inequality
constraints, and nonlinear equality constraints, respectively.  The
number of objective functions must be 1 or greater, and the number of
inequality and equality constraints must be 0 or greater.  When
interfacing to external applications, the responses must be returned
to %Dakota in this order.

The \c sense specification provides strings for declaring
"minimization" or "maximization" (can be shortened to "min" or "max";
not case sensitive) for each of the objective functions, indicating
the goal for each objective within an optimization.  If a single
string is specified it will apply to each objective function.  The
\c primary_scale_types specification includes strings specifying the
scaling type for each objective function value in methods that support
scaling, when scaling is enabled (see \ref MethodIndControl for
details). Each entry in \c primary_scale_types may be selected from
<tt>'none'</tt>, <tt>'value'</tt>, or <tt>'log'</tt>, to select no,
characteristic value, or logarithmic scaling, respectively.  Automatic
scaling is not available for objective functions.  If a single string
is specified it will apply to each objective function.  Each entry in
\c primary_scales may be a user-specified nonzero characteristic value
to be used in scaling each objective function.  These values are
ignored for scaling type <tt>'none'</tt>, required for
<tt>'value'</tt>, and optional for <tt>'log'</tt>.  If a single real
value is specified it will apply to each function.  If the number of
objective functions is greater than 1, then a \c weights specification
provides a simple weighted-sum approach to combining multiple
objectives: \f[f = \sum_{i=1}^{n} w_{i}f_{i}\f] If this is not
specified, then each objective function is given equal weighting: 
\f[f = \sum_{i=1}^{n} \frac{f_i}{n}\f] where, in both of these cases, a
"minimization" sense will retain a positive weighting for a minimizer
and a "maximization" sense will apply a negative weighting.  If
scaling is specified, it is applied before multi-objective weighted
sums are formed.

The \c lower_bounds and \c upper_bounds 
specifications provide the lower and
upper bounds for 2-sided nonlinear inequalities of the form
\f[g_l \leq g(x) \leq g_u\f]
The defaults for the inequality constraint bounds are selected so that 
one-sided inequalities of the form
\f[g(x) \leq 0.0\f]
result when there are no user constraint bounds specifications (this
provides backwards compatibility with previous %Dakota versions). In a
user bounds specification, any upper bound values greater than \c
+bigRealBoundSize (1.e+30, as defined in Minimizer)
are treated as +infinity and any lower bound values less than \c 
-bigRealBoundSize are treated as -infinity.  This feature is commonly 
used to drop one of the bounds in order to specify a 1-sided constraint 
(just as the default lower bounds drop out since \c -DBL_MAX < \c 
-bigRealBoundSize).  The same approach is used for nonexistent linear 
inequality bounds as described in \ref MethodIndControl and for 
nonexistent design variable bounds as described in \ref VarDV.

The \c targets specification provides the targets 
for nonlinear equalities of the form
\f[g(x) = g_t\f]
and the defaults for the equality targets enforce a value of \c 0. 
for each constraint
\f[g(x) = 0.0\f]

The \c scale_types
specifications include strings
specifying the scaling type for each nonlinear inequality or equality
constraint, respectively, in methods that support scaling, when scaling
is enabled (see \ref MethodIndControl for details). Each entry in \c
scale_types may be selected from <tt>'none'</tt>,
<tt>'value'</tt>, <tt>'auto'</tt>, or <tt>'log'</tt>, to select no,
characteristic value, automatic, or logarithmic scaling, respectively.
If a single string is specified it will apply to all components of the
relevant nonlinear constraint vector.  Each entry in \c scales may be a
user-specified nonzero characteristic value to be used in scaling each
constraint component.  These values are ignored for scaling type
<tt>'none'</tt>, required for <tt>'value'</tt>, and optional for
<tt>'auto'</tt> and <tt>'log'</tt>.  If a single real value is
specified it will apply to each constraint.

Any linear constraints present in an application need only be input to
an optimizer at start up and do not need to be part of the data
returned on every function evaluation (see the linear constraints
description in \ref MethodIndControl). \ref T9d3 "Table 9.3"
summarizes the optimization data set specification.

\anchor T9d3
<table>
<caption align = "top">
\htmlonly
Table 9.3
\endhtmlonly
Specification detail for optimization data sets
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Number of objective functions
<td>\c objective_functions
<td>integer
<td>Required group
<td>N/A
<tr>
<td>Optimization sense
<td>\c sense
<td>list of strings
<td>Optional
<td>vector values = <tt>'minimize'</tt>
<tr>
<td>Objective function scaling types
<td>\c primary_scale_types
<td>list of strings
<td>Optional
<td>vector values = <tt>'none'</tt>
<tr>
<td>Objective function scales
<td>\c primary_scales
<td>list of reals
<td>Optional
<td>vector values = \c 1. (no scaling)
<tr>
<td>Multi-objective weightings
<td>\c weights
<td>list of reals
<td>Optional
<td>equal weightings
<tr>
<td>Number of nonlinear inequality constraints
<td>\c nonlinear_inequality_constraints
<td>integer
<td>Optional
<td>\c 0
<tr>
<td>Nonlinear inequality constraint lower bounds
<td>\c lower_bounds
<td>list of reals
<td>Optional
<td>vector values = \c -DBL_MAX
<tr>
<td>Nonlinear inequality constraint upper bounds
<td>\c upper_bounds
<td>list of reals
<td>Optional
<td>vector values = \c 0.
<tr>
<td>Number of nonlinear equality constraints
<td>\c nonlinear_equality_constraints
<td>integer
<td>Optional
<td>\c 0
<tr>
<td>Nonlinear equality constraint targets
<td>\c targets
<td>list of reals
<td>Optional
<td>vector values = \c 0.
<tr>
<td>Nonlinear constraint scaling types (for inequalities or equalities)
<td>\c scale_types
<td>list of strings
<td>Optional
<td>vector values = <tt>'none'</tt>
<tr>
<td>Nonlinear constraint scales (for inequalities or equalities)
<td>\c scales
<td>list of reals
<td>Optional
<td>vector values = \c 1. (no scaling)
</table>


\subsection RespFnLS Calibration terms and constraint functions (least squares data set)

A calibration data set is specified using \c calibration_terms and
optionally the specifications summarized in \ref T9d4 "Table 9.4" and
\ref T9d5 "Table 9.5", including weighting/scaling, data, and
constraints.  Each of the calibration terms is a residual function to
be driven toward zero, and the nonlinear inequality and equality
constraint specifications have identical meanings to those described
in \ref RespFnOpt.  These types of problems are commonly encountered
in parameter estimation, system identification, and model calibration,
where the optimization goal is to find parameters maximizing the
agreement between a computational model and data. Smooth least squares
calibration problems are most efficiently solved using special-purpose
least squares solvers such as Gauss-Newton or Levenberg-Marquardt;
however, they may also be solved using general-purpose optimization
algorithms.

While %Dakota can solve calibration problems with either least squares
or optimization algorithms, the response data returned from the
simulator differ. Least squares calibration involves a set of residual
functions whereas optimization involves a single objective function
(sum of the squares of the residuals), i.e., \f[ f = \sum_{i=1}^{n}
R_i^2 = \sum_{i=1}^{n}{\left( y^M_i - y^O_i \right) ^2} \f] where \e f
is the objective function and the set of \f$R_i\f$ are the residual
functions.  Typically the residuals \f[R_i = y^M_i - y^O_i \f] are the
difference between \e M, model and \e O, observation.  When using a
calibration data set, the user will typically compute the difference
between the model results and the observations, and return the set of
\e n residuals \f$ \{ R_i \} \f$ to %Dakota.  Therefore in the
calibration case, function values and derivative data refer to values
and derivatives of the residual functions, whereas the optimization
case involves values and derivatives of \e f, the sum of squares
objective function.  Switching between the two approaches sometimes
requires different simulation interfaces capable of returning the
different granularity of response data required, although %Dakota
supports automatic recasting of residuals into a sum of squares for
presentation to an optimization method.

However, the user also has the option of specifying the observational
data (e.g., from physical experiments or other sources) in a file.
The specification \c calibration_data_file specifies a text file
containing (in the simplest case) \c calibration_terms observed data
values \f$ y^O_i \f$, in a supported %Dakota tabular format (default
formats change in %Dakota 5.2 -- see User's Manual).  In this case the
simulator should return the actual model responses \f$ y^M_i \f$, as
%Dakota will compute the residuals \f$ R_i \f$ internally using the
supplied data.  In an example of this simple case, the number of model
responses and number provided data might both equal 3.

A more advanced use of the \c calibration_data_file might specify \c
num_experiments \f$ N_E \f$ and \c num_replicates \f$ N_R \f$,
indicating that there are multiple experiments, each with 1 or more
replicates to inform each residual term.  When multiple experiments or
replicates are present, %Dakota will expand the number of residuals
for the repeat measurement data and difference with the data
accordingly.  For example, if the user has one experiment but five
replicates were taken, in the example above with three calibration
terms, the \c calibration_data_file would need to contain five rows
(one for each replicate), and each row should contain three
experimental data values that will be differenced with respect to the
appropriate model response.  In this example, \f$ N_E = 1 \f$, and \f$
N_R = 5\f$.  One can have varying numbers of experiments and
replicates, such as two experiments, one with 3 replicates and one
with 7 replicates.  In this example, \c num_experiments = 2, and the
number of replicates would be given as an integer vector: \c
num_replicates = 3 7.  In this example, there would be a total of 10
rows of data in the experimental data file.  To summarize, %Dakota
will calculate the sum of the squared residuals as: \f[f =
\sum_{i=1}^{N_E}\sum_{j=1}^{{N_R}_i}R_{i,j}^2\f] where the residuals
now are calculated as: \f[R_{i,j} = y^M_i - y^O_{i,j}. \f]

Finally, we have the capability for the user to specify different
values for the experimental error, i.e., measurement or observation
error in the \c calibration_data_file.  The keyword \c
num_std_deviations specifies this.  If \c num_std_deviations = 0, the
user does not specify any experimental error terms in the \c
calibration_data_file, only the actual observations are specified.  If
the user specifies \c num_std_deviations equal to the number of
calibration terms, then each row of a freeform \c
calibration_data_file must contain two times \c calibration_terms.
The first \c calibration_terms columns are the experimental data, and
the second \c calibration_terms columns are the experimental standard
deviations.  For example, if the user has three \c calibration terms,
and specifies \c num_std_deviations = 3, then the calibration data
must contain six columns.  The first three columns will contain the
data, and the second three columns will contain the experimental error
for the data in the first three columns.  Finally, if the user
specifies \c num_std_deviations = 1, the same value of the standard
deviations will be used for all of the calibration terms.  In the
example given above, with three calibration terms, if \c
num_std_deviations = 1, the there would be four columns in the \c
calibration_data_file, and the fourth column would contain the
standard deviation that would be applied to all three calibration
columns.  Note that the standard deviations are given in units of a
standard deviation or sigma term, not a variance term.  These standard
deviations are used to weight the residuals in the sum-of-squares
objective.

When interfacing to external applications, the responses
returned to %Dakota should be ordered calibration terms, inequality
constraints, equality constraints.

The \c primary_scale_types specification includes strings
specifying the scaling type for each residual term in methods that
support scaling, when scaling is enabled (see \ref MethodIndControl
for details). Each entry in \c primary_scale_types may be
selected from <tt>'none'</tt>, <tt>'value'</tt>, or <tt>'log'</tt>, to
select no, characteristic value, or logarithmic scaling, respectively.
Automatic scaling is not available for calibration terms.  If a single
string is specified it will apply to each least squares terms.  Each
entry in \c calibration_term_scales may be a user-specified nonzero
characteristic value to be used in scaling each term.  These values
are ignored for scaling type <tt>'none'</tt>, required for
<tt>'value'</tt>, and optional for <tt>'log'</tt>.  If a single real
value is specified it will apply to each term.  The \c
weights specification provides a means to specify a
relative emphasis among the vector of squared residuals through
multiplication of these squared residuals by a vector of weights: \f[f
= \sum_{i=1}^{n} w_i R_i^2 = \sum_{i=1}^{n} w_i (y^M_i - y^O_i)^2\f]
If characteristic value scaling is additionally specified, then it is
applied to each residual prior to squaring: \f[f = \sum_{i=1}^{n} w_i
(\frac{y^M_i - y^O_i}{s_i})^2\f] And in the case where experimental
data uncertainties are supplied, then the weights are automatically
defined to be the inverse of the experimental variance: \f[f =
\sum_{i=1}^{n} \frac{1}{\sigma^2_i} (\frac{y^M_i - y^O_i}{s_i})^2\f]

\anchor T9d4
<table>
<caption align = "top">
\htmlonly
Table 9.4
\endhtmlonly
Specification detail for nonlinear least squares data sets (calibration terms)
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Number of calibration terms
<td>\c calibration_terms
<td>integer
<td>Required
<td>N/A
<tr>
<td>Calibration data file name
<td>\c calibration_data_file
<td>string
<td>Optional
<td>none
<tr>
<td>Experiments in file
<td>\c num_experiments
<td>integer
<td>Optional
<td>1
<tr>
<td>Replicates per each experiment in file
<td>\c num_replicates
<td>integer vector
<td>Optional
<td>1
<tr>
<td>Data file in annotated format
<td>\c annotated 
<td>boolean
<td>Optional
<td>annotated
<tr>
<td>Data file in freeform format
<td>\c freeform
<td>boolean
<td>Optional
<td>annotated
<tr>
<td>Configuration variable columns in file
<td>\c num_config_variables
<td>integer
<td>Optional
<td>0
<tr>
<td>Standard deviation columns in file
<td>\c num_std_deviations
<td>integer
<td>Optional
<td>0
<tr>
<td>Calibration scaling types
<td>\c primary_scale_types
<td>list of strings
<td>Optional
<td>vector values = <tt>'none'</tt>
<tr>
<td>Calibration scales
<td>\c primary_scales
<td>list of reals
<td>Optional
<td>no scaling (vector values = \c 1.)
<tr>
<td>Calibration term weights
<td>\c weights
<td>list of reals
<td>Optional
<td>equal weighting
</table>

\anchor T9d5
<table>
<caption align = "top">
\htmlonly
Table 9.5
\endhtmlonly
Specification detail for nonlinear least squares data sets (constraints)
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Number of nonlinear inequality constraints
<td>\c nonlinear_inequality_constraints
<td>integer
<td>Optional
<td>\c 0
<tr>
<td>Nonlinear inequality lower bounds
<td>\c lower_bounds
<td>list of reals
<td>Optional
<td>vector values = \c -DBL_MAX
<tr>
<td>Nonlinear inequality upper bounds
<td>\c upper_bounds
<td>list of reals
<td>Optional
<td>vector values = \c 0.
<tr>
<td>Number of nonlinear equality constraints
<td>\c nonlinear_equality_constraints
<td>integer
<td>Optional
<td>\c 0
<tr>
<td>Nonlinear equality targets
<td>\c targets
<td>list of reals
<td>Optional
<td>vector values = \c 0.
<tr>
<td>Nonlinear scaling types (for inequalities or equalities)
<td>\c scale_types
<td>list of strings
<td>Optional
<td>vector values = <tt>'none'</tt>
<tr>
<td>Nonlinear scales (for inequalities or equalities)
<td>\c scales
<td>list of reals
<td>Optional
<td>no scaling (vector values = \c 1.)
</table>


\subsection RespFnGen Response functions (generic data set)

A generic response data set is specified using \c
response_functions. Each of these functions is simply a response
quantity of interest with no special interpretation taken by the
method in use. This type of data set is used by uncertainty
quantification methods, in which the effect of parameter uncertainty
on response functions is quantified, and can also be used in parameter
study and design of experiments methods (although these methods are
not restricted to this data set), in which the effect of parameter
variations on response functions is evaluated. Whereas objective,
constraint, and residual functions have special meanings for
optimization and least squares algorithms, the generic response
function data set need not have a specific interpretation and the user
is free to define whatever functional form is convenient. 
\ref T9d6 "Table 9.6" summarizes the generic response function data 
set specification.

\anchor T9d6
<table>
<caption align = "top">
\htmlonly
Table 9.6
\endhtmlonly
Specification detail for generic response function data sets
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Number of response functions
<td>\c response_functions
<td>integer
<td>Required
<td>N/A
</table>


\section RespGrad Gradient Specification


The gradient specification must be one of four types: 1) no gradients,
2) numerical gradients, 3) analytic gradients, or 4) mixed gradients.


\subsection RespGradNone No gradients

The \c no_gradients specification means that gradient information is
not needed in the study. Therefore, it will neither be retrieved from
the simulation nor computed with finite differences. The \c
no_gradients keyword is a complete specification for this case.


\subsection RespGradNum Numerical gradients

The \c numerical_gradients specification means that gradient
information is needed and will be computed with finite differences
using either the native or one of the vendor finite differencing
routines.

The \c method_source setting specifies the source of the finite
differencing routine that will be used to compute the numerical
gradients: \c dakota denotes %Dakota's internal finite differencing
algorithm and \c vendor denotes the finite differencing algorithm
supplied by the iterator package in use (DOT, CONMIN, NPSOL, NL2SOL, NLSSOL,
and OPT++ each have their own internal finite differencing
routines). The \c dakota routine is the default since it can execute
in parallel and exploit the concurrency in finite difference
evaluations (see Exploiting Parallelism in the Users Manual 
[\ref UsersMan "Adams et al., 2010"]).
However, the \c vendor setting can be desirable in some cases since
certain libraries will modify their algorithm when the finite
differencing is performed internally. Since the selection of the \c
dakota routine hides the use of finite differencing from the
optimizers (the optimizers are configured to accept user-supplied
gradients, which some algorithms assume to be of analytic accuracy),
the potential exists for the \c vendor setting to trigger the use of
an algorithm more optimized for the higher expense and/or lower
accuracy of finite-differencing.  For example, NPSOL uses gradients in
its line search when in user-supplied gradient mode (since it assumes
they are inexpensive), but uses a value-based line search procedure
when internally finite differencing.  The use of a value-based line
search will often reduce total expense in serial operations. However,
in parallel operations, the use of gradients in the NPSOL line search
(user-supplied gradient mode) provides excellent load balancing
without need to resort to speculative optimization approaches.  In
summary, then, the \c dakota routine is preferred for parallel
optimization, and the \c vendor routine may be preferred for serial
optimization in special cases.

When the \c method_source is \c dakota, the user may also specify the
type of scaling desired when determining the finite difference step
size.  The choices are \c absolute, \c bounds, and \c relative.  For
\c absolute, the step size will be applied as is.  For \c bounds, it
will be scaled by the range of each parameter.  For \c relative, it
will be scaled by the parameter value.

The \c interval_type setting is used to select between \c forward and
\c central differences in the numerical gradient calculations. The \c
dakota, DOT \c vendor, and OPT++ \c vendor routines have both forward
and central differences available, the CONMIN and NL2SOL \c vendor routines
support forward differences only, and the NPSOL and NLSSOL \c vendor
routines start with forward differences and automatically switch to
central differences as the iteration progresses (the user has no
control over this).  The following forward difference expression
\f[
\nabla f ({\bf x}) \cong 
\frac{f ({\bf x} + h {\bf e}_i) - f ({\bf x})}{h}
\f]
and the following central difference expression
\f[
\nabla f ({\bf x}) \cong 
\frac{f ({\bf x} + h {\bf e}_i) - f ({\bf x} - h {\bf e}_i)}{2h}
\f]
are used to estimate the \f$i^{th}\f$ component of the gradient vector.  

Lastly, \c fd_gradient_step_size specifies the relative finite
difference step size to be used in the computations.  Either a single
value may be entered for use with all parameters, or a list of step
sizes may be entered, one for each parameter.  The latter option of a
list of step sizes is only valid for use with the %Dakota finite
differencing routine.  For %Dakota with an interval scaling type of \c
absolute, the differencing interval will be \c fd_gradient_step_size.
For %Dakota with and interval scaling type of \c bounds, the
differencing intervals are computed by multiplying \c
fd_gradient_step_size with the range of the parameter. For %Dakota
(with an interval scaling type of \c relative), DOT, CONMIN, and
OPT++, the differencing intervals are computed by multiplying the \c
fd_gradient_step_size with the current parameter value.  In this case,
a minimum absolute differencing interval is needed when the current
parameter value is close to zero.  This prevents finite difference
intervals for the parameter which are too small to distinguish
differences in the response quantities being computed. %Dakota, DOT,
CONMIN, and OPT++ all use <tt>.01*fd_gradient_step_size</tt> as their
minimum absolute differencing interval.  With a
<tt>fd_gradient_step_size = .001</tt>, for example, %Dakota, DOT,
CONMIN, and OPT++ will use intervals of .001*current value with a
minimum interval of 1.e-5.  NPSOL and NLSSOL use a different formula
for their finite difference intervals:
<tt>fd_gradient_step_size*(1+|current parameter value|)</tt>.  This
definition has the advantage of eliminating the need for a minimum
absolute differencing interval since the interval no longer goes to
zero as the current parameter value goes to zero.

When %Dakota computes gradients or Hessians by finite differences and the
variables in question have bounds, it by default chooses finite-differencing
steps that keep the variables within their specified bounds.  Older versions
of %Dakota generally ignored bounds when computing finite differences.
To restore the older behavior, one can add keyword <tt>ignore_bounds</tt>
to the <tt>response</tt> specification when <tt>method_source dakota</tt>
(or just <tt>dakota</tt>) is also specified.
In forward difference or backward difference computations, honoring
bounds is straightforward.
To honor bounds when approximating \f$\partial f / \partial x_i\f$, i.e., component \f$i\f$
of the gradient of \f$f\f$, by central differences, %Dakota chooses two steps
\f$h_1\f$ and \f$h_2\f$ with \f$h_1 \ne h_2\f$, such that \f$x + h_1 e_i\f$
and \f$x + h_2 e_i\f$ both satisfy the bounds, and then computes
\f[
\frac{\partial f}{\partial x_i} \cong
\frac{h_2^2(f_1 - f_0) - h_1^2(f_2 - f_0)}{h_1 h_2 (h_2 - h_1)} ,
\f]
with \f$f_0 = f(x)\f$, \f$f_1 = f(x + h_1 e_i)\f$, and
\f$f_2 = f(x + h_2 e_i)\f$.

  \ref T9d7 "Table 9.7" summarizes the numerical
gradient specification.

\anchor T9d7
<table>
<caption align = "top">
\htmlonly
Table 9.7
\endhtmlonly
Specification detail for numerical gradients
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Numerical gradients
<td>\c numerical_gradients
<td>none
<td>Required group
<td>N/A
<tr>
<td>Method source
<td>\c method_source
<td>\c dakota | \c vendor
<td>Optional group
<td>\c dakota
<tr>
<td>Interval scaling type
<td>\c dakota
<td>\c absolute | \c bounds | \c relative
<td>Optional group
<td>\c relative
<tr>
<td>Interval type
<td>\c interval_type
<td>\c forward | \c central
<td>Optional group
<td>\c forward
<tr>
<td>Finite difference step size
<td>\c fd_gradient_step_size
<td>list of reals
<td>Optional
<td><tt>0.001</tt>
<tr>
<td>Ignore variable bounds
<td>ignore_bounds
<td>none
<td>Optional
<td>bounds respected
</table>


\subsection RespGradAnalytic Analytic gradients

The \c analytic_gradients specification means that gradient
information is available directly from the simulation (finite
differencing is not required). The simulation must return the gradient
data in the %Dakota format (enclosed in single brackets; see %Dakota
File Data Formats in the Users Manual [\ref UsersMan "Adams et al., 2010"]) 
for the case of file transfer of data. The \c analytic_gradients keyword 
is a complete specification for this case.


\subsection RespGradMixed Mixed gradients

The \c mixed_gradients specification means that some gradient
information is available directly from the simulation (analytic)
whereas the rest will have to be finite differenced (numerical). This
specification allows the user to make use of as much analytic gradient
information as is available and then finite difference for the
rest. For example, the objective function may be a simple analytic
function of the design variables (e.g., weight) whereas the
constraints are nonlinear implicit functions of complex analyses
(e.g., maximum stress). The \c id_analytic_gradients list specifies by
number the functions which have analytic gradients, and the \c
id_numerical_gradients list specifies by number the functions which
must use numerical gradients. Each function identifier, from 1 through
the total number of functions, must appear once and only once within
the union of the \c id_analytic_gradients and \c
id_numerical_gradients lists.  The \c method_source, \c interval_type,
and \c fd_gradient_step_size specifications are as described
previously in \ref RespGradNum and pertain to those functions listed
by the \c id_numerical_gradients list. \ref T9d8 "Table 9.8"
summarizes the mixed gradient specification.

\anchor T9d8
<table>
<caption align = "top">
\htmlonly
Table 9.8
\endhtmlonly
Specification detail for mixed gradients
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Mixed gradients
<td>\c mixed_gradients
<td>none
<td>Required group
<td>N/A
<tr>
<td>Analytic derivatives function list
<td>\c id_analytic_gradients
<td>list of integers
<td>Required
<td>N/A
<tr>
<td>Numerical derivatives function list
<td>\c id_numerical_gradients
<td>list of integers
<td>Required
<td>N/A
<tr>
<td>Method source
<td>\c method_source
<td>\c dakota | \c vendor
<td>Optional group
<td>\c dakota
<tr>
<td>Interval scaling type
<td>\c dakota
<td>\c absolute | \c bounds | \c relative
<td>Optional group
<td>\c relative
<tr>
<td>Interval type
<td>\c interval_type
<td>\c forward | \c central
<td>Optional group
<td>\c forward
<tr>
<td>Finite difference step size
<td>\c fd_step_size
<td>list of reals
<td>Optional
<td><tt>0.001</tt>
<tr>
<td>Ignore variable bounds
<td>\c ignore_bounds
<td>none
<td>Optional
<td>bounds respected
</table>


\section RespHess Hessian Specification


Hessian availability must be specified with either \c no_hessians, \c
numerical_hessians, \c quasi_hessians, \c analytic_hessians, or \c
mixed_hessians.


\subsection RespHessNone No Hessians

The \c no_hessians specification means that the method does not
require %Dakota to manage the computation of any Hessian
information. Therefore, it will neither be retrieved from the
simulation nor computed by %Dakota. The \c no_hessians keyword is a
complete specification for this case.  Note that, in some cases,
Hessian information may still be being approximated internal to an
algorithm (e.g., within a quasi-Newton optimizer such as \c
optpp_q_newton); however, %Dakota has no direct involvement in this
process and the responses specification need not include it.


\subsection RespHessNum Numerical Hessians

The \c numerical_hessians specification means that Hessian information
is needed and will be computed with finite differences using either
first-order gradient differencing (for the cases of \c
analytic_gradients or for the functions identified by \c
id_analytic_gradients in the case of \c mixed_gradients) or
first- or second-order function value differencing (all other gradient
specifications).  In the former case, the following expression
\f[
\nabla^2 f ({\bf x})_i \cong 
\frac{\nabla f ({\bf x} + h {\bf e}_i) - \nabla f ({\bf x})}{h}
\f]
estimates the \f$i^{th}\f$ Hessian column, and in the latter case, the
following expressions
\f[
\nabla^2 f ({\bf x})_{i,j} \cong \frac{f({\bf x} + h_i {\bf e}_i + h_j {\bf e}_j) - 
f({\bf x} + h_i {\bf e}_i) - 
f({\bf x} - h_j {\bf e}_j) + 
f({\bf x})}{h_i h_j}
\f]
and
\f[
\nabla^2 f ({\bf x})_{i,j} \cong \frac{f({\bf x} + h {\bf e}_i + h {\bf e}_j) - 
f({\bf x} + h {\bf e}_i - h {\bf e}_j) - 
f({\bf x} - h {\bf e}_i + h {\bf e}_j) + 
f({\bf x} - h {\bf e}_i - h {\bf e}_j)}{4h^2}
\f]
provide first- and second-order estimates of the \f$ij^{th}\f$ Hessian term.
Prior to %Dakota 5.0, %Dakota always used second-order estimates.
In %Dakota 5.0 and newer, the default is to use first-order estimates
(which honor bounds on the variables and
require only about a quarter as many function evaluations
as do the second-order estimates), but specifying <tt>central</tt>
after <tt>numerical_hessians</tt> causes %Dakota to use the old second-order
estimates, which do not honor bounds.  In optimization algorithms that
use Hessians, there is little reason to use second-order differences in
computing Hessian approximations.

The \c fd_hessian_step_size specifies the relative finite difference
step size to be used in these differences.  Either a single value may
be entered for use with all parameters, or a list of step sizes may be
entered, one for each parameter.  When the interval scaling type is \c
absolute, the differencing intervals are \c fd_hessian_step_size.
When the interval scaling type is \c bounds, the differencing
intervals are computed by multiplying the \c fd_hessian_step_size with
the range of the parameter.  When the interval scaling type is \c
relative, the differencing intervals are computed by multiplying the
\c fd_hessian_step_size with the current parameter value.  A minimum
absolute differencing interval of <tt>.01*fd_hessian_step_size</tt> is
used when the current parameter value is close to zero.  \ref T9d9
"Table 9.9" summarizes the numerical Hessian specification.

\anchor T9d9
<table>
<caption align = "top">
\htmlonly
Table 9.9
\endhtmlonly
Specification detail for numerical Hessians
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Numerical Hessians
<td>\c numerical_hessians
<td>none
<td>Required group
<td>N/A
<tr>
<td>Interval scaling type
<td>\c absolute | \c bounds | \c relative
<td>\c none
<td>Optional
<td>\c relative
<tr>
<td>Finite difference step size
<td>\c fd_step_size
<td>list of reals
<td>Optional
<td><tt>0.001</tt> (1st-order), <tt>0.002</tt> (2nd-order)
<tr>
<td>Difference order
<td>\c forward | \c central
<td>none
<td>Optional
<td>forward
</table>


\subsection RespHessQuasi Quasi Hessians

The \c quasi_hessians specification means that Hessian information is
needed and will be approximated using secant updates (sometimes called
"quasi-Newton updates", though any algorithm that approximates
Newton's method is a quasi-Newton method).
Compared to finite difference numerical Hessians, secant
approximations do not expend additional function evaluations in
estimating all of the second-order information for every point of
interest.  Rather, they accumulate approximate curvature information
over time using the existing gradient evaluations.  The supported
secant approximations include the
Broyden-Fletcher-Goldfarb-Shanno (BFGS) update (specified with the
keyword \c bfgs)

\f[
B_{k+1} = B_{k} - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + 
\frac{y_k y_k^T}{y_k^T s_k}
\f]

and the Symmetric Rank 1 (SR1) update (specified with the keyword \c sr1)

\f[
B_{k+1} = B_k + \frac{(y_k - B_k s_k)(y_k - B_k s_k)^T}{(y_k - B_k s_k)^T s_k}
\f]

where \f$B_k\f$ is the \f$k^{th}\f$ approximation to the Hessian, 
\f$s_k = x_{k+1} - x_k\f$ is the step and 
\f$y_k = \nabla f_{k+1} - \nabla f_k\f$ is the corresponding yield 
in the gradients.  In both cases, an initial scaling of 
\f$\frac{y_k^T y_k}{y_k^T s_k} I\f$ is used for \f$B_0\f$ prior to the first 
update.  In addition, both cases employ basic numerical safeguarding 
to protect against numerically small denominators within the updates.  
This safeguarding skips the update if 
\f$|y_k^T s_k| < 10^{-6} s_k^T B_k s_k\f$ in the BFGS case or if 
\f$|(y_k - B_k s_k)^T s_k| < 10^{-6} ||s_k||_2 ||y_k - B_k s_k||_2\f$ 
in the SR1 case.  In the BFGS case, additional safeguarding can be 
added using the \c damped option, which utilizes an alternative 
damped BFGS update when the curvature condition \f$y_k^T s_k > 0\f$ 
is nearly violated.  \ref T9d10 "Table 9.10" summarizes the quasi 
Hessian specification.

\anchor T9d10
<table>
<caption align = "top">
\htmlonly
Table 9.10
\endhtmlonly
Specification detail for quasi Hessians
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Quasi Hessians
<td>\c quasi_hessians
<td>\c bfgs | \c sr1
<td>Required group
<td>N/A
<tr>
<td>Numerical safeguarding of BFGS update
<td>\c damped
<td>none
<td>Optional
<td>undamped BFGS
</table>


\subsection RespHessAnalytic Analytic Hessians

The \c analytic_hessians specification means that Hessian information
is available directly from the simulation. The simulation must return
the Hessian data in the %Dakota format (enclosed in double brackets; see
%Dakota File Data Formats in Users Manual 
[\ref UsersMan "Adams et al., 2010"]) for the case of file transfer of 
data. The \c analytic_hessians keyword is a complete specification for 
this case.


\subsection RespHessMixed Mixed Hessians

The \c mixed_hessians specification means that some Hessian
information is available directly from the simulation (analytic)
whereas the rest will have to be estimated by finite differences
(numerical) or approximated by secant updating. As for
mixed gradients, this specification allows the user to make use of as
much analytic information as is available and then
estimate/approximate the rest. The \c id_analytic_hessians list
specifies by number the functions which have analytic Hessians, and
the \c id_numerical_hessians and \c id_quasi_hessians lists specify by
number the functions which must use numerical Hessians and
secant Hessian updates, respectively. Each function identifier,
from 1 through the total number of functions, must appear once and
only once within the union of the \c id_analytic_hessians, \c
id_numerical_hessians, and \c id_quasi_hessians lists.  The \c
fd_hessian_step_size and \c bfgs, \c damped \c bfgs, or \c sr1
secant update selections are as described previously in \ref
RespHessNum and \ref RespHessQuasi and pertain to those functions
listed by the \c id_numerical_hessians and \c id_quasi_hessians
lists. \ref T9d11 "Table 9.11" summarizes the mixed Hessian
specification.

\anchor T9d11
<table>
<caption align = "top">
\htmlonly
Table 9.11
\endhtmlonly
Specification detail for mixed Hessians
</caption>
<tr>
<td><b>Description</b>
<td><b>Keyword</b>
<td><b>Associated Data</b>
<td><b>Status</b>
<td><b>Default</b>
<tr>
<td>Mixed Hessians
<td>\c mixed_hessians
<td>none
<td>Required group
<td>N/A
<tr>
<td>Analytic Hessians function list
<td>\c id_analytic_hessians
<td>list of integers
<td>Required
<td>N/A
<tr>
<td>Numerical Hessians function list
<td>\c id_numerical_hessians
<td>list of integers
<td>Required
<td>N/A
<tr>
<td>Interval scaling type
<td>\c absolute | \c bounds | \c relative
<td>\c none
<td>Optional
<td>\c relative
<tr>
<td>Finite difference step size
<td>\c fd_step_size
<td>list of reals
<td>Optional
<td><tt>0.001</tt> (1st-order), <tt>0.002</tt> (2nd-order)
<tr>
<td>Quasi Hessians function list
<td>\c id_quasi_hessians
<td>list of integers
<td>Required
<td>N/A
<tr>
<td>Quasi-Hessian update
<td>\c bfgs | \c sr1
<td>none
<td>Required
<td>N/A
<tr>
<td>Numerical safeguarding of BFGS update
<td>\c damped
<td>none
<td>Optional
<td>undamped BFGS
</table>

\htmlonly
<hr>
<br><b><a href="InterfCommands.html#InterfCommands">Previous chapter</a></b>
<br>
<br><b><a href="Bibliography.html#Bibliography">Next chapter</a></b>
\endhtmlonly

*/

} // namespace Dakota
