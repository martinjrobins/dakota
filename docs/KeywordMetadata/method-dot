Blurb::
Access to methods in the DOT package
Description::
The DOT library 
\cite Van95 
contains nonlinear programming optimizers, specifically the
Broyden-Fletcher-Goldfarb-Shanno (%Dakota's \c dot_bfgs method) and
Fletcher-Reeves conjugate gradient (%Dakota's \c dot_frcg method)
methods for unconstrained optimization, and the modified method of
feasible directions (%Dakota's \c dot_mmfd method), sequential linear
programming (%Dakota's \c dot_slp method), and sequential quadratic
programming (%Dakota's \c dot_sqp method) methods for constrained
optimization.

Specialized handling of linear constraints is
supported with DOT; linear constraint coefficients, bounds, and
targets can be provided to DOT at start-up and tracked
internally. 

One of the five available methods in <b> Group 1 </b> must be specified.

All these methods take the same <b> Optional Keywords </b>, dealing with
linear equality and inequality constraints.

<b> Method Independent Controls - Stopping Critiera </b>

Stopping critiera are set by:
\li \ref method-dot-max_iterations 
\li \ref method-dot-max_function_evaluations
\li \ref method-dot-convergence_tolerance
\li \ref method-dot-constraint_tolerance 

Note: The \c convergence_tolerance criterion must be satisfied
    for two consecutive iterations before DOT will terminate. 

<b> Method Independent Controls - Output </b>

The output verbosity
specification controls the amount of information generated by DOT: the
\c silent and \c quiet settings result in header information, final
results, and objective function, constraint, and parameter information
on each iteration; whereas the \c verbose and \c debug settings add
additional information on gradients, search direction, one-dimensional
search results, and parameter scaling factors. 

<b> Concurrency </b>

DOT contains no
parallel algorithms which can directly take advantage of concurrent
evaluations. However, if \c numerical_gradients with \c method_source
\c dakota is specified, then the finite difference function
evaluations can be performed concurrently (using any of the parallel
modes described in the Users Manual \cite UsersMan). 
In addition, if \c speculative
is specified, then gradients (\c dakota \c numerical or \c analytic
gradients) will be computed on each line search evaluation in order to
balance the load and lower the total run time in parallel optimization
studies. 


Topics::	package_dot
Examples::
Theory::
Faq::
See_Also::	
